@online{CiteWorth,
  title = {{{CiteWorth}}: {{Cite-Worthiness Detection}} for {{Improved Scientific Document Understanding}}},
  shorttitle = {{{CiteWorth}}},
  author = {Wright, Dustin and Augenstein, Isabelle},
  date = {2021-05-26},
  eprint = {2105.10912},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2105.10912},
  url = {http://arxiv.org/abs/2105.10912},
  urldate = {2025-11-13},
  abstract = {Scientific document understanding is challenging as the data is highly domain specific and diverse. However, datasets for tasks with scientific text require expensive manual annotation and tend to be small and limited to only one or a few fields. At the same time, scientific documents contain many potential training signals, such as citations, which can be used to build large labelled datasets. Given this, we present an in-depth study of cite-worthiness detection in English, where a sentence is labelled for whether or not it cites an external source. To accomplish this, we introduce CiteWorth, a large, contextualized, rigorously cleaned labelled dataset for cite-worthiness detection built from a massive corpus of extracted plain-text scientific documents. We show that CiteWorth is high-quality, challenging, and suitable for studying problems such as domain adaptation. Our best performing cite-worthiness detection model is a paragraph-level contextualized sentence labelling model based on Longformer, exhibiting a 5 F1 point improvement over SciBERT which considers only individual sentences. Finally, we demonstrate that language model fine-tuning with cite-worthiness as a secondary task leads to improved performance on downstream scientific document understanding tasks.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Digital Libraries,Computer Science - Machine Learning},
  file = {/Users/cjduhamel/Zotero/storage/LHDHCNHG/Wright and Augenstein - 2021 - CiteWorth Cite-Worthiness Detection for Improved Scientific Document Understanding.pdf;/Users/cjduhamel/Zotero/storage/QZAU7HTC/2105.html}
}

@online{HuggingFace,
  title = {Hugging {{Face}} – {{The AI}} Community Building the Future.},
  date = {2025-12-09},
  url = {https://huggingface.co/},
  urldate = {2025-12-11},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
  file = {/Users/cjduhamel/Zotero/storage/VXI8KY8V/huggingface.co.html}
}

@article{pandas,
  title = {Pandas: A {{Foundational Python Library}} for {{Data Analysis}} and {{Statistics}}},
  shorttitle = {Pandas},
  author = {Mckinney, Wes},
  date = {2011-01-01},
  journaltitle = {Python High Performance Science Computer},
  shortjournal = {Python High Performance Science Computer},
  abstract = {—In this paper we will discuss pandas, a Python library of rich data structures and tools for working with structured data sets common to statistics, finance, social sciences, and many other fields. The library provides integrated, intuitive routines for performing common data manipulations and analysis on such data sets. It aims to be the foundational layer for the future of statistical computing in Python. It serves as a strong complement to the existing scientific Python stack while implementing and improving upon the kinds of data manipulation tools found in other statistical programming languages such as R. In addition to detailing its design and features of pandas, we will discuss future avenues of work and growth opportunities for statistics and data analysis applications in the Python language.},
  file = {/Users/cjduhamel/Zotero/storage/SLAMZK3Q/Mckinney - 2011 - pandas a Foundational Python Library for Data Analysis and Statistics.pdf}
}

@inproceedings{S2AG,
  title = {The {{Semantic Scholar Academic Graph}} ({{S2AG}})},
  booktitle = {Companion {{Proceedings}} of the {{Web Conference}} 2022},
  author = {Wade, Alex D.},
  date = {2022-08-16},
  series = {{{WWW}} '22},
  pages = {739},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3487553.3527147},
  url = {https://dl.acm.org/doi/10.1145/3487553.3527147},
  urldate = {2025-12-10},
  abstract = {The Semantic Scholar Academic Graph, or S2AG (pronounced ”stag”), is a large, open, heterogeneous knowledge graph of scholarly works, authors, and citations that powers the Semantic Scholar discovery service. S2AG currently contains over 205M publications, 121M authors, and nearly 2.5B citation edges. Semantic Scholar integrates metadata from Crossref, PubMed, Unpaywall, and other sources. In addition, through partnerships with academic publishers and through web-crawling, we source and process the full-text of nearly 60M full-text publications in order extract and classify the document structure, including references, citation contexts, figures, tables, and more. S2AG is available via an open API as well as via downloadable monthly snapshots. In this talk, we will describe the S2AG resource as well as the Semantic Scholar Open Research Corpus, or S2ORC (pronounced ”stork”), a general purpose, multi-domain corpus for NLP and text mining research.},
  isbn = {978-1-4503-9130-6},
  file = {/Users/cjduhamel/Zotero/storage/CZ8JMDBF/Wade - 2022 - The Semantic Scholar Academic Graph (S2AG).pdf}
}

@inproceedings{S2ORC,
  title = {{{S2ORC}}: {{The Semantic Scholar Open Research Corpus}}},
  shorttitle = {{{S2ORC}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Lo, Kyle and Wang, Lucy Lu and Neumann, Mark and Kinney, Rodney and Weld, Daniel},
  date = {2020-07},
  pages = {4969--4983},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2020.acl-main.447},
  url = {https://aclanthology.org/2020.acl-main.447/},
  urldate = {2025-11-13},
  abstract = {We introduce S2ORC, a large corpus of 81.1M English-language academic papers spanning many academic disciplines. The corpus consists of rich metadata, paper abstracts, resolved bibliographic references, as well as structured full text for 8.1M open access papers. Full text is annotated with automatically-detected inline mentions of citations, figures, and tables, each linked to their corresponding paper objects. In S2ORC, we aggregate papers from hundreds of academic publishers and digital archives into a unified source, and create the largest publicly-available collection of machine-readable academic text to date. We hope this resource will facilitate research and development of tools and tasks for text mining over academic text.},
  eventtitle = {{{ACL}} 2020},
  file = {/Users/cjduhamel/Zotero/storage/PR5UMSFJ/Lo et al. - 2020 - S2ORC The Semantic Scholar Open Research Corpus.pdf}
}


@inproceedings{sciBERT,
  title = {{{SciBERT}}: {{A Pretrained Language Model}} for {{Scientific Text}}},
  shorttitle = {{{SciBERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Beltagy, Iz and Lo, Kyle and Cohan, Arman},
  editor = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
  date = {2019-11},
  pages = {3615--3620},
  publisher = {Association for Computational Linguistics},
  location = {Hong Kong, China},
  doi = {10.18653/v1/D19-1371},
  url = {https://aclanthology.org/D19-1371/},
  urldate = {2025-12-11},
  abstract = {Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.},
  eventtitle = {{{EMNLP-IJCNLP}} 2019},
  file = {/Users/cjduhamel/Zotero/storage/L5XHQ9UU/Beltagy et al. - 2019 - SciBERT A Pretrained Language Model for Scientific Text.pdf}
}
