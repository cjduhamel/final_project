\documentclass{article}

\usepackage[utf8]{inputenc}

% use si uints packege soren was yappin about

\usepackage[backend=biber, style=ieee]{biblatex}
\addbibresource{../final.bib} % <-- your .bib file

% I put together a bunch of packages that you'll need/want in 445.sty.  Feel
% free to take a look!
\usepackage{basic_482} % thank you Brian Jones for the template
\setterm{Fall}
\setclass{CSC 482}


\usepackage{setspace}
\usepackage{array}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{bm}
\usepackage{subcaption}
\captionsetup{font=small,labelfont=bf}

\usepackage{minted}

\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows.meta, positioning}
\usetikzlibrary{mindmap, shadows}
\tikzstyle{process} = [rectangle, minimum width=4cm, minimum height=1.2cm, text centered, draw=black, fill=blue!10]
\tikzstyle{arrow} = [thick, -{Stealth}]

\begin{document}

\createtitle{Project Report - Auto Citations}{C.J. DuHamel and Arian Houshmand}

\section{Introduction}
Citations are essential to academia, facilitating collaboration, aknowledging prior work, and providing a basis for further research. However, managing citations can be very time-consuming, requiring the writer to manually format, organize, and place citations within their work, taking aluable time away from the actual writing and research that goes into the paper. To address this challenge, we created an Auto Citation tool that automates the placing of in-text citations given a list of referenced works. We aim to streamline the citation process, reducing workload on researchers and reducing the spread of plaigerism by ensuring proper attribution. 



\subsection{Problem Details}
The goal of this project is to develop a tool that automatically inserts in-text citations into a document based on a provided bibliography/collection of referenced works. Ideally, the tool should be able to analyze the un-cited document and identify the optimal placements for in-text citations, ensuring that they are contextually relevant to the content being discussed.

We will focus on a subset of the full functionality that would be desired for a complete tool. Specifically, we will target paragraph level citation placement, identifying for each reference the most relevant paragraph in the document to place the citation. To do this, we will use various natural language processing (NLP) techniques, specifically leveraging transformer-based models to utilize modern methods of semantic understanding of text.

\section{Data Sources}
\subsection{CiteWorth}
CiteWorth \cite{CiteWorth} is a dataset of academic papers compiled by researches at the University of Copenhagen. The dataset contains 1.2 million cleaned sentences from academic papers, labeled with their 'cite-worthiness', or whether or not that sentence contains an in-text citation. The dataset is not only labeled with cite-worthiness, but also annotated heavily with details about the cited papers and various representations of the sentences themselves.

Each entry in the dataset represents a paragraph from an academic paper, containing a variety of fields, including:
\begin{itemize}
    \item \textbf{paper\_id}: The unique identifier for the paper from which the paragraph was extracted.
    \item \textbf{original\_text}: The actual text of the paragraph.
    \item \textbf{section\_title}: The title of the section in which the paragraph appears.
    \item \textbf{samples}: A list of sentences within the paragraph, each with its own set of annotations.
    \begin{itemize}
        \item \textbf{text}: The cleaned sentence text without the citations.
        \item \textbf{label}: A binary label indicating whether the sentence contains a citation (1) or not (0).
        \item \textbf{original\_text}: The original sentence text with citations included.
        \item \textbf{ref\_ids}: A list of reference IDs corresponding to the citations present in the sentence. These are the 'Corpus Ids' used to uniquely identify cited papers in the Semantic Scholar Open Research Corpus (S2ORC) dataset.
        \item \textbf{citation\_text}: List of citation strings as they appear in the original text.
    \end{itemize}
\end{itemize}

\subsection{Semantic Scholar Open Research Corpus}
The Semantic Scholar Open Research Corpus (S2ORC) \cite{S2ORC} is a large dataset of academic papers (400+ GB) compiled by researchers at the Allen Institute for AI. The dataset contains over 81 million papers, including their full text and metadata for natural language processing tasks. Each paper in the dataset is assigned a unique 'Corpus Id', which is used to identify and reference papers within the dataset.

The dataset is organized in JSONL format, with each line representing a single paper. Each paper entry contains various fields, including:
\begin{itemize}
    \item \textbf{paper\_id}: The unique identifier for the paper.
    \item \textbf{metadata}: Metadata about the paper, including title, authors, abstract, and publication venue.
    \item \textbf{body (or content)}: All relevant information from the paper, including the full text, figure and table info/captions, and any included annotations (byte offsets, in-text citation locations, etc.). This section contains a lot of information and is losely structured. It soemtimes contains nested fields, other times the fields are called different names, and sometimes the information is missing altogether. However, the full text of the paper is ussually in a consistent format.
    \item \textbf{bibliography}: A list of references cited in the paper, each with its own set of metadata, including the 'matched\_paper\_id', which is the id for the referenced paper in the Semantic Scholar Academic Graph (S2AG) \cite{S2AG}, a superset of the S2ORC that contains significantly more papers, but not all of them contain text. This also contains a section called 'annotations', which contains the matched paper ids and byte offsets for bibliography entries.
    
\end{itemize}

\section{System Design Overview}
Our system is comprised of several key components that make up the pipeline for auto placing in text citations. We begin with the data sources mentioned above, which will be the basis for our project. The dataset retrieval is a major step in the process. Once the data is retrieved, we create our own local database to facilitate faster lookups during dataset building. Next, we build our training dataset from the combined CiteWorth and S2ORC datasets. This dataset will then be used to train and evaluate our model. Finally, we will utilize the model for inference on new complete documents to place citations automatically.

\section{Dataset Creation and Design}
Building a high-quality dataset is crucial to model performance. Ideally, we want to take advantage of the large datasets available, since we have to handle them in their entirety anyway. However, we need to ensure that the data we have is both relevant and mostly contained within the S2ORC dataset, since not all referenced papers have full text available (i.e. they are only in the S2AG dataset).

\subsection{Data Retrieval}

Initially, we had assumed that the S2ORC dataset could be accessed via API calls to retrieve individual papers and their data. We were, however, mistaken. Thus, we are required to download the full S2ORC dataset in bulk and extract the relevant papers using their 'Corpus Id's. This is significantly more complex than we had originally anticipated, and much time has been spent just designing the downloader. 

The general algorithm we wrote for downloading the dataset is as follows:
\begin{enumerate}
    \item Call the Semantic Scholar (S2) API to retrieve the most recent release of the S2ORC dataset.
    \item Call the S2 API again to retrieve the presigned download links for the dataset files.
    \item Build a CSV by extracting the "shard\_id" for each download link and associating it with the corresponding presigned URL. Add additonal columns to the CSV to track download status and local file paths. This is to facilitate resuming downloads in case of interruptions.
    \item Download the dataset files using the presigned links. For each link (i.e. row in the CSV), do the following:
        \begin{enumerate}
            \item Check the download status column in the CSV to see if the file has already been downloaded. If it has, skip to the next row.
            \item If the file has not been downloaded, download it and save it to the specified local file path.
            \item After a successful download, update the download status column in the CSV to indicate that the file has been downloaded.
            \item The presigned URLs have a limited validity period, which is much less than the time required to download the whole dataset. If a link has expired, refresh the links by repeating step 2 and updating the CSV with the new links. Then, retry downloading the expired file.
        \end{enumerate}
\end{enumerate}

Retrieving the CiteWorth dataset was significantly simpler, as it is available directly via Hugging Face \cite{HuggingFace}. We simply downloaded the dataset via their API and pandas \cite{pandas} for easy exploration and manipulation.

\subsection{Dataset Building}
We begin with a mapping between paper corpus\_ids to the file paths in the S2ORC dataset that we created for the last checkpoint. This mapping is stored in a CSV file where each row contains a corpus\_id and the corresponding file path (shard) to the S2ORC JSON file. We then read in the CiteWorth dataset and group by the corpus\_id to create a list of all the sections in the dataset for each paper. We do the following for each paper in the CiteWorth dataset:
\begin{enumerate}
    \item If the corpus\_id does not exist in the S2ORC mapping, we skip the paper.
    \item Fetch the S2ORC JSON object using the file path from the mapping.
    \item For each section in the CiteWorth dataset for the paper:
    \begin{enumerate}
        \item Identify the bibliography reference IDs that are present in that section
        \item For each reference ID, locate the corresponding corpus\_id in the S2ORC JSON object's bibliography.
        \item Get each referenced paper's JSON object using the corpus\_id to shard mapping.
    \end{enumerate}
    \item Reiterate through each sample (sentence) in each section:
    \begin{enumerate}
        \item If there are no references in the sample, skip it.
        \item For each reference, create a new entry in the structured dataset that contains:
        \begin{itemize}
            \item The original paper's corpus\_id
            \item The sentence text from the original paper
            \item The reference paper's corpus\_id
            \item The reference paper's title
            \item The reference paper's authors
            \item The first 500 characters of the reference paper's text (either abstract or introduction)
            \item A label indicating whether the reference paper should be cited in that sentence (1 for cite, 0 for no cite)
        \end{itemize}
    \end{enumerate}

\item Write the new json object to a JSONL file, along with writing the original paper's corpus\_id to a .log file to allow for the script to be resumed if interrupted.
\end{enumerate}

The process creates a dataset of sentence - reference pairs, with a label indicating whether the reference should be cited in that sentence. This structured dataset can then be used for training and evaluating our auto citation model.

\subsection{Example Entries}
Here are some example entries from the structured dataset:
\begin{minted}[frame=lines, bgcolor=gray!10, breaklines=true]{json}
    {
        "original_paper_id": 386802, 
        "sentence": "Clinical antigen-specific studies however, have so far failed to show the desired efficacy despite their good safety profile.", 
        "ref_paper_id": 713469, 
        "ref_paper_title": "Non-antigenic and antigenic interventions in type 1 diabetes", 
        "ref_paper_authors": "Anna Ryd\u00e9n, J. Wesley, K. Coppieters, M. V. von Herrath", 
        "ref_paper_text": "\nIntroduction\n\nDiabetes mellitus describes the outcome of several metabolic disorders characterized by hyperglycemia, including type 1 diabetes (T1D). In the context of T1D, hyperglycemia typically results from an immunologically driven assault on the \u03b2-cells-the insulin-producing cells of the pancreas-leading to insufficient insulin secretion. 4 \u03b2-Cell destruction is often rapid in young subjects but more prolonged in adults; this rate, however, is subject to great variability between individua", 
        "label": 0
    }
    {
        "original_paper_id": 386802, 
        "sentence": "Clinical antigen-specific studies however, have so far failed to show the desired efficacy despite their good safety profile.", 
        "ref_paper_id": 18952695, 
        "ref_paper_title": "Immunotherapy for the Prevention and Treatment of Type 1 Diabetes", 
        "ref_paper_authors": "M. Rewers, P. Gottlieb", 
        "ref_paper_text": "\nI\n\nn the past 15 years, multiple clinical trials have attempted to find prevention for type 1 diabetes. The accompanying article by Bresson and von Herrath (1) reviews basic mechanisms underlying immunoprevention and immunotherapy of type 1 diabetes as well as selected human trials in the context of data from animal models. The second part of this minisymposium provides an overview of the recent or ongoing human trials. Immunotherapy for prevention of type 1 diabetes or to ameliorate the course", 
        "label": 1
    }
\end{minted}

\section{The Model}
Used SciBERT \cite{sciBERT}

\section{Results and Evaluation}

\subsection{Evaluation Metrics}

\section{Limitations and Problems}
While we were working on our project, we encountered a variety of limitations that required workarounds, patience, or simply accepting defeat. The major limitations we faced were:

\subsection{S2ORC Dataset Size}
The S2ORC dataset is extremely large, at over 500 GB in size. As outlined in the Dataset Retrieval section, downloading the dataset itself was a significant portion of our time spent on the project. Additionally, there was no cloud storage solution available for free to store 500 GB of data, so we had to store it locally on CJ's personal computer. This severely limited our ability to collaborate effectively, as only one of us could access the full S2ORC dataset. We were, however, able to work around this by splitting up the work in a different way.

The size was not just a factor in storage and download time, but also in processing time. Searching through the dataset, even after building our local mapping, was still quite slow, despite our best efforts to parallelize the work. Not only did this slow down our dataset building process, requiring about 2 weeks to build the full dataset, but it also restricted CJ's ability to utilize his personal computer for other tasks while the dataset building was running. This resulted in trading off between data aquisition and the usage of his PC, which is used as a remote machine for all of his projects. On average, it took about 5-7 minutes to identify a usable entry from the CiteWorth dataset and extract all relevant information from the S2ORC dataset, Thus resulting in over 100 hours of continuous processing time to build the full dataset.

\subsection{Parsing Issues with S2ORC}
The S2ORC dataset is not perfectly structured, and there are many inconsistencies in the way the data is parsed and stored. For example, some papers have their main text stored within a field called "body" that contains a variety of inconsistent nested subfields. Other papers have their main text stored in a field called "content", which is structured differently. Additionally, while many papers contain annotations for the locations of in-text citations, bibliography entries, and other useful information, many papers are missing this information or have limited annotations, so this method was not a reliable way of extracting the necessary data.

In addition, while all papers that have paragraph entries in the CiteWorth dataset have corresponding entries in the S2ORC dataset, not all referenced papers in the CiteWorth dataset have full text available in the S2ORC dataset, thus was not in the S2ORC dataset at all. This made it difficult to build a complete dataset, as we had to skip many entries that still required a portion of the processing time to determine that they were unusable. This may, however, have a silver lining, as it forced our dataset to contain a larger bredth of citation examples, rather than many examples from fewer papers. This may help our model generalize better to unseen papers during inference, as it was trained on a wider variety of examples.

\subsection{Corruption...}
During the building of the test dataset, CJ's computer crashed. Upon restart we received the error shown in figure \ref{fig:crash}. This error seems to indicate that the filesystem was corrupted. This drive is now approximately 8 years old, so it may be that it has reached the end of its lifespan. Unfortunately, this meant that all the data on the drive was lost, including the entirety of the S2ORC dataset, the built index, the partially built test dataset, and all of the additional logic that was written to build the test dataset (not to mention over a TB of personal data). Despite numerous attempts at repairing the drive, including restarts, opening in other OSs, and running drive repair utilities, we were unable to regain access to the data on the drive. Fortunately, all code prior to building the test set was in GitHub, so not all was lost. 

We unfortunately did not have time or storage capacity to re-download the S2ORC dataset and re-build the test set, so we worked around this by hand parsing a single paper that was not contained in any of our current datasets (it is actually a paper that is closely related to CJ's thesis research). While this is not ideal and is very time consuming, it allows us to at least demonstrate the inference capabilities of our model.



\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/crashed.png}
    \caption{Error message after CJ's computer crashed during test set building.}
    \label{fig:crash}
\end{figure}


\section{Conclusion and Future Work}

\printbibliography

\end{document}