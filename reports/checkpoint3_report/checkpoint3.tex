\documentclass{article}

\usepackage[utf8]{inputenc}

% use si uints packege soren was yappin about

\usepackage[backend=biber, style=ieee]{biblatex}
\addbibresource{../final.bib} % <-- your .bib file

% I put together a bunch of packages that you'll need/want in 445.sty.  Feel
% free to take a look!
\usepackage{basic_482} % thank you Brian Jones for the template
\setterm{Fall}
\setclass{CSC 482}


\usepackage{setspace}
\usepackage{array}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{bm}
\usepackage{subcaption}
\captionsetup{font=small,labelfont=bf}

\usepackage{minted}

\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows.meta, positioning}
\usetikzlibrary{mindmap, shadows}
\tikzstyle{process} = [rectangle, minimum width=4cm, minimum height=1.2cm, text centered, draw=black, fill=blue!10]
\tikzstyle{arrow} = [thick, -{Stealth}]

\begin{document}

\createtitle{Project Checkpoint 3}{C.J. DuHamel and Arian Houshmand}

\section{Structured Dataset Creation}

Last checkpoint, we discussed the creation of our local database of academic papers from the S2ORC dataset. Since then we have sucessfully created a structured dataset that combines the important features from the CiteWorth \cite{CiteWorth} dataset with the associated papers from the S2ORC dataset \cite{S2ORC}. The creation of the structured dataset is outlined as follows:

\subsection{Dataset Creation Algorithm}
We begin with a mapping between paper corpus\_ids to the file paths in the S2ORC dataset that we created for the last checkpoint. This mapping is stored in a CSV file where each row contains a corpus\_id and the corresponding file path (shard) to the S2ORC JSON file. We then read in the CiteWorth dataset and group by the corpus\_id to create a list of all the sections in the dataset for each paper. We do the following for each paper in the CiteWorth dataset:
\begin{enumerate}
    \item If the corpus\_id does not exist in the S2ORC mapping, we skip the paper.
    \item Fetch the S2ORC JSON object using the file path from the mapping.
    \item For each section in the CiteWorth dataset for the paper:
    \begin{enumerate}
        \item Identify the bibliography reference IDs that are present in that section
        \item For each reference ID, locate the corresponding corpus\_id in the S2ORC JSON object's bibliography.
        \item Get each referenced paper's JSON object using the corpus\_id to shard mapping.
    \end{enumerate}
    \item Reiterate through each sample (sentence) in each section:
    \begin{enumerate}
        \item If there are no references in the sample, skip it.
        \item For each reference, create a new entry in the structured dataset that contains:
        \begin{itemize}
            \item The original paper's corpus\_id
            \item The sentence text from the original paper
            \item The reference paper's corpus\_id
            \item The reference paper's title
            \item The reference paper's authors
            \item The first 500 characters of the reference paper's text (either abstract or introduction)
            \item A label indicating whether the reference paper should be cited in that sentence (1 for cite, 0 for no cite)
        \end{itemize}
    \end{enumerate}

\item Write the new json object to a JSONL file, along with writing the original paper's corpus\_id to a .log file to allow for the script to be resumed if interrupted.
\end{enumerate}

The process creates a dataset of sentence - reference pairs, with a label indicating whether the reference should be cited in that sentence. This structured dataset can then be used for training and evaluating our auto citation model.

\subsection{Challenges and Limitations}
The process of searching through the S2ORC dataset to find a particular paper takes a very long time. We have implemented a parallel approach with minimal overhead to speed up the process. But, fetching a paper from S2ORC takes on average about 20-30 seconds. This makes developing and testing the dataset creation script very slow and lengthy. Upon completion, the script takes, on average 8 minutes to find a single valid paper with at least one findable reference in S2ORC. Each found paper can yield anywhere from around 25 entries to just 1 entry in the structured dataset, with a majority of papers yielding less than 10 entries.

This severely limits the amount of data we can create in the time frame of the project. In addition, the script can only be run C.J.'s personal computer, as that is where all of the data is stored. Since this pc is used for other tasks, projects, and general use, it is not feasible to run the script for long periods of time.

In addition, we also found that many referenced paper corpus\_ids are not present in the S2ORC dataset, which we believe is due to the fact that the corpus\_ids that are associated with the referenced papers correspond to papers located in the S2AG dataset, a superset of S2ORC that contains additional papers that do not have full text available. This further limits the amount of data we can create for our structured dataset.

\subsection{Example Entries}
Here are some example entries from the structured dataset:
\begin{minted}[frame=lines, bgcolor=gray!10, breaklines=true]{json}
    {
        "original_paper_id": 386802, 
        "sentence": "Clinical antigen-specific studies however, have so far failed to show the desired efficacy despite their good safety profile.", 
        "ref_paper_id": 713469, 
        "ref_paper_title": "Non-antigenic and antigenic interventions in type 1 diabetes", 
        "ref_paper_authors": "Anna Ryd\u00e9n, J. Wesley, K. Coppieters, M. V. von Herrath", 
        "ref_paper_text": "\nIntroduction\n\nDiabetes mellitus describes the outcome of several metabolic disorders characterized by hyperglycemia, including type 1 diabetes (T1D). In the context of T1D, hyperglycemia typically results from an immunologically driven assault on the \u03b2-cells-the insulin-producing cells of the pancreas-leading to insufficient insulin secretion. 4 \u03b2-Cell destruction is often rapid in young subjects but more prolonged in adults; this rate, however, is subject to great variability between individua", 
        "label": 0
    }
    {
        "original_paper_id": 386802, 
        "sentence": "Clinical antigen-specific studies however, have so far failed to show the desired efficacy despite their good safety profile.", 
        "ref_paper_id": 18952695, 
        "ref_paper_title": "Immunotherapy for the Prevention and Treatment of Type 1 Diabetes", 
        "ref_paper_authors": "M. Rewers, P. Gottlieb", 
        "ref_paper_text": "\nI\n\nn the past 15 years, multiple clinical trials have attempted to find prevention for type 1 diabetes. The accompanying article by Bresson and von Herrath (1) reviews basic mechanisms underlying immunoprevention and immunotherapy of type 1 diabetes as well as selected human trials in the context of data from animal models. The second part of this minisymposium provides an overview of the recent or ongoing human trials. Immunotherapy for prevention of type 1 diabetes or to ameliorate the course", 
        "label": 1
    }
\end{minted}

\section{Initial Neural Network Model}

\subsection{Model Overview}
With the structured dataset created, we have begun work on an initial neural network model for the auto citation task. The model is a binary classification model that takes as input the sentence - reference pair and outputs a prediction of whether the reference should be cited in the sentence (1 for cite, 0 for no cite).

Upon inference (given a reference, find the best sentence to cite it in), the model will run on all sentences in the paper and output a probability for each sentence. The sentences will then be ranked and a top N sentences can be selected for citation, with N varying based on user preference.

\subsection{Model Architecture}

\subsection{Model Results}


\printbibliography

\end{document}