\documentclass{article}

\usepackage[utf8]{inputenc}

% use si uints packege soren was yappin about

\usepackage[backend=biber, style=ieee]{biblatex}
\addbibresource{../final.bib} % <-- your .bib file

% I put together a bunch of packages that you'll need/want in 445.sty.  Feel
% free to take a look!
\usepackage{basic_482} % thank you Brian Jones for the template
\setterm{Fall}
\setclass{CSC 482}


\usepackage{setspace}
\usepackage{array}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{bm}
\usepackage{subcaption}
\captionsetup{font=small,labelfont=bf}

\usepackage{minted}

\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows.meta, positioning}
\usetikzlibrary{mindmap, shadows}
\tikzstyle{process} = [rectangle, minimum width=4cm, minimum height=1.2cm, text centered, draw=black, fill=blue!10]
\tikzstyle{arrow} = [thick, -{Stealth}]

\begin{document}

\createtitle{Project Checkpoint 3}{C.J. DuHamel and Arian Houshmand}

\section{Structured Dataset Creation}

Last checkpoint, we discussed the creation of our local database of academic papers from the S2ORC dataset. Since then we have sucessfully created a structured dataset that combines the important features from the CiteWorth \cite{CiteWorth} dataset with the associated papers from the S2ORC dataset \cite{S2ORC}. The creation of the structured dataset is outlined as follows:

\subsection{Dataset Creation Algorithm}
We begin with a mapping between paper corpus\_ids to the file paths in the S2ORC dataset that we created for the last checkpoint. This mapping is stored in a CSV file where each row contains a corpus\_id and the corresponding file path (shard) to the S2ORC JSON file. We then read in the CiteWorth dataset and group by the corpus\_id to create a list of all the sections in the dataset for each paper. We do the following for each paper in the CiteWorth dataset:
\begin{enumerate}
    \item If the corpus\_id does not exist in the S2ORC mapping, we skip the paper.
    \item Fetch the S2ORC JSON object using the file path from the mapping.
    \item For each section in the CiteWorth dataset for the paper:
    \begin{enumerate}
        \item Identify the bibliography reference IDs that are present in that section
        \item For each reference ID, locate the corresponding corpus\_id in the S2ORC JSON object's bibliography.
        \item Get each referenced paper's JSON object using the corpus\_id to shard mapping.
    \end{enumerate}
    \item Reiterate through each sample (sentence) in each section:
    \begin{enumerate}
        \item If there are no references in the sample, skip it.
        \item For each reference, create a new entry in the structured dataset that contains:
        \begin{itemize}
            \item The original paper's corpus\_id
            \item The sentence text from the original paper
            \item The reference paper's corpus\_id
            \item The reference paper's title
            \item The reference paper's authors
            \item The first 500 characters of the reference paper's text (either abstract or introduction)
            \item A label indicating whether the reference paper should be cited in that sentence (1 for cite, 0 for no cite)
        \end{itemize}
    \end{enumerate}

\item Write the new json object to a JSONL file, along with writing the original paper's corpus\_id to a .log file to allow for the script to be resumed if interrupted.
\end{enumerate}

The process creates a dataset of sentence - reference pairs, with a label indicating whether the reference should be cited in that sentence. This structured dataset can then be used for training and evaluating our auto citation model.

\subsection{Challenges and Limitations}
The process of searching through the S2ORC dataset to find a particular paper takes a very long time. We have implemented a parallel approach with minimal overhead to speed up the process. But, fetching a paper from S2ORC takes on average about 20-30 seconds. This makes developing and testing the dataset creation script very slow and lengthy. Upon completion, the script takes, on average 8 minutes to find a single valid paper with at least one findable reference in S2ORC. Each found paper can yield anywhere from around 25 entries to just 1 entry in the structured dataset, with a majority of papers yielding less than 10 entries.

This severely limits the amount of data we can create in the time frame of the project. In addition, the script can only be run C.J.'s personal computer, as that is where all of the data is stored. Since this pc is used for other tasks, projects, and general use, it is not feasible to run the script for long periods of time.

In addition, we also found that many referenced paper corpus\_ids are not present in the S2ORC dataset, which we believe is due to the fact that the corpus\_ids that are associated with the referenced papers correspond to papers located in the S2AG dataset, a superset of S2ORC that contains additional papers that do not have full text available. This further limits the amount of data we can create for our structured dataset.

\subsection{Example Entries}
Here are some example entries from the structured dataset:
\begin{minted}[frame=lines, bgcolor=gray!10, breaklines=true]{json}
    {
        "original_paper_id": 386802, 
        "sentence": "Clinical antigen-specific studies however, have so far failed to show the desired efficacy despite their good safety profile.", 
        "ref_paper_id": 713469, 
        "ref_paper_title": "Non-antigenic and antigenic interventions in type 1 diabetes", 
        "ref_paper_authors": "Anna Ryd\u00e9n, J. Wesley, K. Coppieters, M. V. von Herrath", 
        "ref_paper_text": "\nIntroduction\n\nDiabetes mellitus describes the outcome of several metabolic disorders characterized by hyperglycemia, including type 1 diabetes (T1D). In the context of T1D, hyperglycemia typically results from an immunologically driven assault on the \u03b2-cells-the insulin-producing cells of the pancreas-leading to insufficient insulin secretion. 4 \u03b2-Cell destruction is often rapid in young subjects but more prolonged in adults; this rate, however, is subject to great variability between individua", 
        "label": 0
    }
    {
        "original_paper_id": 386802, 
        "sentence": "Clinical antigen-specific studies however, have so far failed to show the desired efficacy despite their good safety profile.", 
        "ref_paper_id": 18952695, 
        "ref_paper_title": "Immunotherapy for the Prevention and Treatment of Type 1 Diabetes", 
        "ref_paper_authors": "M. Rewers, P. Gottlieb", 
        "ref_paper_text": "\nI\n\nn the past 15 years, multiple clinical trials have attempted to find prevention for type 1 diabetes. The accompanying article by Bresson and von Herrath (1) reviews basic mechanisms underlying immunoprevention and immunotherapy of type 1 diabetes as well as selected human trials in the context of data from animal models. The second part of this minisymposium provides an overview of the recent or ongoing human trials. Immunotherapy for prevention of type 1 diabetes or to ameliorate the course", 
        "label": 1
    }
\end{minted}

\section{Initial Neural Network Model}

\subsection{Model Overview}
With the structured dataset created, we have begun work on an initial neural network model for the auto citation task. Each data point in our dataset is a \emph{sentence--reference} pair:
\begin{itemize}
    \item a sentence $s$ from an original paper, and
    \item a candidate reference $r$ (another paper),
\end{itemize}
together with a binary label $y \in \{0,1\}$ indicating whether $r$ should be cited in $s$ ($y=1$ for \texttt{cite}, $y=0$ for \texttt{no\_cite}).

We cast this as a binary classification problem: given $(s, r)$, the model predicts the probability that the reference is actually being cited in that sentence. At inference time (e.g., given a reference and all sentences in a paper), we can:
\begin{enumerate}
    \item score each sentence--reference pair with the model,
    \item interpret the output as $P(\text{cite} \mid s, r)$, and
    \item rank sentences by this probability to select the top $N$ sentences as suggested citation locations.
\end{enumerate}

\subsection{Model Architecture}
We use a SciBERT-based cross-encoder as our core model.

\paragraph{Base model.}
We start with a BERT-style transformer pretrained on a large corpus of scientific articles. SciBERT \cite{sciBERT} is well-suited to our setting because both the sentences and the reference texts come from scientific papers.

\paragraph{Input representation.}
Each raw example includes:
\begin{itemize}
    \item \texttt{sentence}: the sentence from the original paper,
    \item \texttt{ref\_paper\_text}: main text for the reference (e.g., abstract or introduction snippet),
    \item optionally (if availble) \texttt{ref\_paper\_title} and \texttt{ref\_paper\_authors},
    \item \texttt{label} $\in \{0,1\}$.
\end{itemize}

From these fields we construct a single reference string:
\[
r_{\text{block}} = \texttt{ref\_paper\_text}
\;+\; \text{optional ``Title: title''}
\;+\; \text{optional ``Authors: authors''},
\]
always placing \texttt{ref\_paper\_text} first since it is the most informative and consistently present field.



\paragraph{Classification head and objective.}
On top of the final hidden state corresponding to the \texttt{[CLS]} token, we add a linear classification head that outputs two logits corresponding to the classes \texttt{NOT\_CITED} and \texttt{CITED}. We apply a softmax to obtain class probabilities and train the entire model end-to-end using a standard cross-entropy loss:
\[
\mathcal{L} = - \big( y \log p_1 + (1-y)\log p_0 \big),
\]
where $p_1$ is the predicted probability of \texttt{CITED} and $p_0$ is the probability of \texttt{NOT\_CITED}.

\paragraph{Handling class imbalance.}
The raw dataset is imbalanced, with many more \texttt{NOT\_CITED} examples than \texttt{CITED} ones. To mitigate this, we perform oversampling of the positive class ($y=1$) in the \emph{training} set only, duplicating positive examples until the counts of positives and negatives are roughly equal. The validation set is left unchanged to reflect the true distribution of labels.

\paragraph{Training configuration.}
We fine-tune SciBERT using the HuggingFace \texttt{Trainer} with the following hyperparameters:
\begin{itemize}
    \item learning rate $2 \times 10^{-5}$,
    \item batch size 8,
    \item 4 training epochs,
    \item weight decay 0.01.
\end{itemize}


\subsection{Model Results}
We evaluate the model on a held-out validation set consisting of sentence--reference pairs that were not used for training. The main metrics are:

\begin{itemize}
    \item overall accuracy $\approx 0.74$,
    \item F1 score for the \texttt{CITED} class $\approx 0.52$.
\end{itemize}

The confusion matrix on the validation set 

\begin{center}
\begin{tabular}{c|cc}
    & \textbf{Pred 0 (NOT\_CITED)} & \textbf{Pred 1 (CITED)} \\
    \hline
    \textbf{True 0 (NOT\_CITED)} & 72 & 15 \\
    \textbf{True 1 (CITED)}      & 17 & 17 \\
\end{tabular}
\end{center}

This shows that the model correctly identifies most non-citation pairs while also recovering a substantial fraction of true citation pairs, despite the relatively small dataset size and inherent difficulty of the task.

\paragraph{Example inference.}

\begin{quote}
\emph{``Specifically, IL-5 is the most important mediator that regulates eosinophilic inflammation through its effects on the proliferation, differentiation, and activation of eosinophils.''}
\end{quote}

When paired with a reference about \emph{``Anti-interleukin-5 therapy in severe asthma''}, the model assigns a high probability to the \texttt{CITED} class. In contrast, when paired with an unrelated reference about \emph{``Smoking cessation and weight gain''}, the model assigns a high probability to \texttt{NOT\_CITED}. This behavior matches our intuition that the model is learning to detect whether the content of the sentence aligns with the content of the proposed reference.

\subsection{Anticipated Usage}
When we want to use the model (given a reference, find the best sentence to cite it in), the model will run on all sentences in the paper and output a probability for each sentence. The sentences will then be ranked and a top N sentences can be selected for citation, with N varying based on user preference.





\printbibliography

\end{document}